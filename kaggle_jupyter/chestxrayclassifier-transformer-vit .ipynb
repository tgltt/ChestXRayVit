{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # 计算训练集数据均值、方差\n# import numpy as np\n# import cv2\n# import random\n\n# import os\n# from tqdm.notebook import tqdm_notebook\n\n# img_root = \"/kaggle/input/chest-xray-pneumoniacovid19tuberculosis/train\"\n# sub_dirs = [os.path.join(img_root, sub_dir) for sub_dir in os.listdir(path=img_root)]\n\n# CNum = 6000  # 挑选多少图片进行计算\n\n# img_h, img_w = 256, 256\n# imgs = np.zeros([img_w, img_h, 1, 1])\n# means, stdevs = [], []\n\n# data_files = []\n# for sub_dir in sub_dirs:\n#     data_files.extend([os.path.join(sub_dir, data_type) for data_type in os.listdir(path=sub_dir)])\n\n# random.shuffle(data_files)  # shuffle, 随机挑选图片\n# for index in tqdm_notebook(range(CNum)):\n#     if index >= len(data_files):\n#         break\n#     data_file = data_files[index]\n#     img = cv2.imread(data_file)\n#     img = cv2.resize(img, (img_h, img_w))\n#     img = img.transpose(2, 0, 1)\n#     img = img.mean(axis=0)\n#     img = img[:, :, np.newaxis, np.newaxis]\n\n#     imgs = np.concatenate((imgs, img), axis=3)\n\n# imgs = imgs.astype(np.float32) / 255.\n\n# for i in tqdm_notebook(range(1)):\n#     pixels = imgs[:, :, i, :].ravel()  # 拉成一行\n#     means.append(np.mean(pixels))\n#     stdevs.append(np.std(pixels))\n\n# # cv2 读取的图像格式为BGR，PIL/Skimage读取到的都是RGB不用转\n# means.reverse()  # BGR --> RGB\n# stdevs.reverse()\n\n# print(\"normMean = {}\".format(means))\n# print(\"normStd = {}\".format(stdevs))\n# print('transforms.Normalize(normMean = {}, normStd = {})'.format(means, stdevs))\n\n# # normMean = [0.48698336]\n# # normStd = [0.23673548]\n# # transforms.Normalize(normMean = [0.48698336], normStd = [0.23673548])","metadata":{"execution":{"iopub.status.busy":"2023-03-19T06:15:58.513733Z","iopub.execute_input":"2023-03-19T06:15:58.515395Z","iopub.status.idle":"2023-03-19T06:15:58.521930Z","shell.execute_reply.started":"2023-03-19T06:15:58.515350Z","shell.execute_reply":"2023-03-19T06:15:58.520706Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# data.py\n\nimport os\nimport torch\nimport json\n\nfrom PIL import Image\n\nTRAIN_DATA_PATH = \"train\"\nVAL_DATA_PATH = \"val\"\nTEST_DATA_PATH = \"test\"\n\nCLASS_NORMAL_KEY = \"NORMAL\"\nCLASS_PNEUMONIA_KEY = \"PNEUMONIA\"\nCLASS_COVID19_KEY = \"COVID19\"\nCLASS_TUBERCULOSIS_KEY = \"TURBERCULOSIS\"\n\nCLASS_COUNT = 4\n\nTARGET_FIELD_LABELS = \"labels\"\nTARGET_FIELD_IMAGE_ID = \"image_id\"\nTARGET_FIELD_HEIGHT_WIDTH = \"height_width\"\nTARGET_FIELD_IMAGE_DATA = \"image_data\"\n\n# 定义数据集\nclass ChestXRayDataset(torch.utils.data.Dataset):\n    _SUPPORT_FILE_TYPES = (\".jpg\", \".jpeg\", \".mpo\", \".png\")\n\n    def __init__(self, data_path, transforms):\n        super(ChestXRayDataset, self).__init__()\n        self.data_path = data_path if len(data_path) > 0 else \".\"\n        self.sub_dirs = [os.path.join(self.data_path, sub_dir) for sub_dir in os.listdir(path=data_path)]\n        # read class_indict\n        json_file = \"/kaggle/input/chest-x-ray-classes/chest_x_ray_classes.json\"\n        assert os.path.exists(json_file), \"{} file not exist.\".format(json_file)\n\n        with open(json_file, 'r') as f:\n            self.class_dict = json.load(f)\n\n        self.data_filenames = []\n        self.label_type = []\n        for sub_dir in self.sub_dirs:\n            label_type = os.path.basename(sub_dir)\n            data_file_names = [os.path.join(data_path, sub_dir, data_file_name) for data_file_name in os.listdir(path=os.path.join(data_path, sub_dir))\n                                if self._is_support_file_type(filename=data_file_name)]\n            self.data_filenames.extend(data_file_names)\n            self.label_type.extend([self.class_dict[label_type] for _ in range(len(data_file_names))])\n\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.data_filenames)\n\n    def __getitem__(self, i):\n        assert i < len(self.data_filenames), f\"Index {i} out of bounds.\"\n        return self._get_data(data_file_name=self.data_filenames[i], label_type=self.label_type[i])\n\n    def _get_data(self, data_file_name, label_type):\n        try:\n            # data_file_name = 'D:\\\\workspace\\\\ai_study\\\\dataset\\\\ChestX-Ray\\\\ChestXRay\\\\train\\\\COVID19\\\\COVID19(436).jpg'\n            raw_image_data = Image.open(fp=data_file_name, mode=\"r\")\n            if not self._is_file_ext_supported(\".\" + raw_image_data.format):\n                raise ValueError(f\"Image '{data_file_name}'s' format not JPEG\")\n\n            target = {}\n            target[TARGET_FIELD_IMAGE_DATA] = raw_image_data\n            target[TARGET_FIELD_IMAGE_ID] = data_file_name\n            target[TARGET_FIELD_LABELS] = torch.as_tensor([label_type], dtype=torch.int64)\n            target[TARGET_FIELD_HEIGHT_WIDTH] = [raw_image_data.height, raw_image_data.width]\n\n            if self.transforms is not None:\n                image, target = self.transforms(raw_image_data, target)\n            return image, target[TARGET_FIELD_LABELS]\n        except:\n            print(f\"Open {data_file_name} failed.\")\n            return (None, None)\n    \n    @staticmethod\n    def collate_fn(batch):\n        images, targets = tuple(zip(*batch))\n\n        filter_images = []\n        filter_targets = []\n        for index in range(len(images)):\n            if images[index] is not None:\n                filter_images.append(images[index])\n                filter_targets.append(targets[index])\n\n        return tuple(filter_images), tuple(filter_targets)\n\n    def _is_file_ext_supported(self, file_ext):\n        return file_ext.lower() in self._SUPPORT_FILE_TYPES\n\n    def _is_support_file_type(self, filename):\n        return os.path.splitext(p=(filename.lower()))[-1] in self._SUPPORT_FILE_TYPES\n\ndef get_chest_xray_dataloader(data_root_path,\n                              data_use_type,\n                              transforms,\n                              batch_size,\n                              drop_last,\n                              shuffle,\n                              collate_fn):\n    dataset = ChestXRayDataset(os.path.join(data_root_path, data_use_type), transforms)\n    # 数据加载器\n    return torch.utils.data.DataLoader(dataset=dataset,\n                                       batch_size=batch_size,\n                                       drop_last=drop_last,\n                                       shuffle=shuffle,\n                                       collate_fn=collate_fn if collate_fn is not None else dataset.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T06:15:58.532208Z","iopub.execute_input":"2023-03-19T06:15:58.532965Z","iopub.status.idle":"2023-03-19T06:15:58.560594Z","shell.execute_reply.started":"2023-03-19T06:15:58.532925Z","shell.execute_reply":"2023-03-19T06:15:58.559270Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# util.py\n\nfrom collections import defaultdict, deque\n\nimport torch\nfrom torch import nn\nfrom torch import Tensor\n\nDEFAULT_HIDDEN_SIZE = 768\nIMAGE_DEFAULT_INNER_SIZE = 256\nPATCH_DEFAULT_SIZE = 16\nIMAGE_DEFAULT_CHANNELS = 3\n\n# 注意力计算函数\n# [BatchSize, Head_i, SeqLen, Emb_Size]\ndef attention(Q, K, V, mask, max_len, emb_size):\n    # b句话,每句话50个词,每个词编码成32维向量,4个头,每个头分到8维向量\n    # Q,K,V = [b, 4, 50, 8]\n\n    # [b, 4, 256 + 1, 192] * [b, 4, 192, 256 + 1] -> [b, 4, 256 + 1, 256 + 1]\n    # Q,K矩阵相乘,求每个词相对其他所有词的注意力\n    score = torch.matmul(Q, K.permute(0, 1, 3, 2))\n\n    # 除以每个头维数的平方根,做数值缩放\n    score /= 8 ** 0.5\n\n    # mask 遮盖,mask是true的地方都被替换成-inf,这样在计算softmax的时候,-inf会被压缩到0\n    # mask = [b, 1, 256 + 1, 256 + 1]\n    if mask is not None:\n        score = score.masked_fill_(mask, -float('inf'))\n\n    score = torch.softmax(score, dim=-1)\n\n    # 以注意力分数乘以V,得到最终的注意力结果\n    # [b, 4, 256 + 1, 256 + 1] * [b, 4, 256 + 1, 192] -> [b, 4, 256 + 1, 192]\n    score = torch.matmul(score, V)\n\n    # 每个头计算的结果合一\n    # [b, 4, 256 + 1, 192] -> [b, 256 + 1, 768]\n    score = score.permute(0, 2, 1, 3).reshape(-1, max_len, emb_size)\n\n    return score\n\n\n# 多头注意力计算层\nclass MultiHead(nn.Module):\n    def __init__(self, max_len, emb_size):\n        super().__init__()\n        self.max_len = max_len\n        self.emb_size = emb_size\n        # Q 矩阵\n        self.fc_Q = nn.Linear(emb_size, emb_size)\n        # K 矩阵\n        self.fc_K = nn.Linear(emb_size, emb_size)\n        # V 矩阵\n        self.fc_V = nn.Linear(emb_size, emb_size)\n\n        self.out_fc = nn.Linear(emb_size, emb_size)\n        #\n        self.norm = nn.LayerNorm(normalized_shape=emb_size, elementwise_affine=True)\n\n        self.dropout = nn.Dropout(p=0.1)\n\n    def forward(self, Q, K, V, mask):\n        # Q, K, V 指的是 embedding + pe 之后的结果\n        # b句话,每句话50个词,每个词编码成32维向量\n        # Q,K,V = [b, 256 + 1, 768]\n\n        # 批量\n        b = Q.shape[0]\n\n        # 保留下原始的Q,后面要做短接用\n        clone_Q = Q.clone()\n\n        # 规范化\n        Q = self.norm(Q)\n        K = self.norm(K)\n        V = self.norm(V)\n\n        # 线性运算,维度不变\n        # [b, 256 + 1, 768] -> [b, 256 + 1, 768]\n        K = self.fc_K(K)\n        V = self.fc_V(V)\n        Q = self.fc_Q(Q)\n\n        # 拆分成多个头\n        # b句话,每句话50个词,每个词编码成32维向量,4个头,每个头分到8维向量\n        # [b, 256 + 1, 768] -> [b, 4, 256 + 1, 192]\n        Q = Q.reshape(b, self.max_len, 4, self.emb_size // 4).permute(0, 2, 1, 3)\n        K = K.reshape(b, self.max_len, 4, self.emb_size // 4).permute(0, 2, 1, 3)\n        V = V.reshape(b, self.max_len, 4, self.emb_size // 4).permute(0, 2, 1, 3)\n\n        # 计算注意力\n        # [b, 4, 256 + 1, 192] -> [b, 256 + 1, 768]\n        score = attention(Q, K, V, mask, self.max_len, self.emb_size)\n\n        # 计算输出,维度不变\n        # [b, 256 + 1, 768] -> [b, 256 + 1, 768]\n        score = self.dropout(self.out_fc(score))\n\n        # 短接\n        score = clone_Q + score\n        return score\n\n# 位置编码层\nclass PatchEmbedding(nn.Module):\n    def __init__(self,\n                 in_channels=IMAGE_DEFAULT_CHANNELS,\n                 patch_size=PATCH_DEFAULT_SIZE,\n                 emb_size=DEFAULT_HIDDEN_SIZE,\n                 img_size=IMAGE_DEFAULT_INNER_SIZE):\n        super(PatchEmbedding, self).__init__()\n        self.patch_size = patch_size\n        self.projection = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels,\n                      out_channels=emb_size,\n                      kernel_size=patch_size,\n                      stride=patch_size)\n        )\n        self.max_seq_len = (img_size // patch_size) ** 2 + 1\n        self.emb_size = emb_size\n        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n        self.positions = nn.Parameter(torch.randn(self.max_seq_len, emb_size))\n\n    def forward(self, x: Tensor) -> Tensor:\n        b, _, _, _ = x.shape\n        # [B, C, H, W] -> [B, C', H', W']\n        x = self.projection(x)\n        # [B, C', H', W'] -> [B, H', W', C']\n        x = x.permute(0, 2, 3, 1)\n        # [B, H', W', C'] -> [B, PATCHES_COUNT, C']\n        x = x.view(b, -1, self.emb_size)\n        # [1, 1, C'] -> [B, 1, C']\n        cls_tokens = self.cls_token.repeat(b, 1, 1)\n        # [B, PATCHES_COUNT, C'] -> [B, PATCHES_COUNT + 1, C']\n        x = torch.cat([cls_tokens, x], dim=1)\n        # 融入位置编码信息\n        x += self.positions\n\n        return x\n\nclass ClassificationHead(nn.Module):\n    def __init__(self, emb_size= DEFAULT_HIDDEN_SIZE, n_classes=CLASS_COUNT):\n        super(ClassificationHead, self).__init__()\n        self.classification_head = nn.Sequential(\n            nn.LayerNorm(normalized_shape=emb_size),\n            nn.Linear(in_features=emb_size, out_features=n_classes)\n        )\n\n    def forward(self, x: Tensor) -> Tensor:\n        # [B, PATCHES_COUNT, C'] -> [B, C']\n        x = x.mean(dim=1)\n        return self.classification_head(x)\n\n# 全连接输出层\nclass FullyConnectedOutput(nn.Module):\n    def __init__(self, hidden_size=DEFAULT_HIDDEN_SIZE):\n        super().__init__()\n        self.fc = torch.nn.Sequential(\n            torch.nn.Linear(in_features=hidden_size, out_features=2 * hidden_size),\n            torch.nn.ReLU(),\n            torch.nn.Linear(in_features=2 * hidden_size, out_features=hidden_size),\n            torch.nn.Dropout(p=0.1)\n        )\n\n        self.norm = torch.nn.LayerNorm(normalized_shape=hidden_size,\n                                       elementwise_affine=True)\n\n    def forward(self, x):\n        # 保留下原始的x,后面要做短接用\n        clone_x = x.clone()\n\n        # 规范化\n        x = self.norm(x)\n\n        # 线性全连接运算\n        # [b, 256 + 1, 768] -> [b, 256 + 1, 768]\n        out = self.fc(x)\n\n        # 做短接\n        out = clone_x + out\n\n        return out","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-19T06:15:58.643972Z","iopub.execute_input":"2023-03-19T06:15:58.644456Z","iopub.status.idle":"2023-03-19T06:15:58.679983Z","shell.execute_reply.started":"2023-03-19T06:15:58.644414Z","shell.execute_reply":"2023-03-19T06:15:58.678711Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# transforms.py\n\nimport random\n\nimport torchvision.transforms as t\nfrom torchvision.transforms import functional as F\n\nclass Compose(object):\n    \"\"\"组合多个transform函数\"\"\"\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target=None):\n        for trans in self.transforms:\n            image, target = trans(image, target)\n        return image, target\n\n\nclass ToTensor(object):\n    \"\"\"将PIL图像转为Tensor\"\"\"\n    def __call__(self, image, target):\n        image = F.to_tensor(image).contiguous()\n        channel_count = len(image)\n        if channel_count > 1:\n            image = image.mean(dim=0).unsqueeze(dim=0)\n        return image, target\n\n\nclass RandomHorizontalFlip(object):\n    \"\"\"随机水平翻转图像以及bboxes,该方法应放在ToTensor后\"\"\"\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            image = image.flip(-1)  # 水平翻转图片\n        return image, target\n\n\nclass SSDCropping(object):\n    \"\"\"\n    对图像进行裁剪,该方法应放在ToTensor前\n    \"\"\"\n    def __init__(self):\n        super(SSDCropping, self).__init__()\n        self.sample_options = (None, (0.3, 1.0))\n\n    def __call__(self, image, target):\n        # 死循环，确保一定会返回结果\n        while True:\n            mode = random.choice(self.sample_options)\n            if mode is None:  # 不做随机裁剪处理\n                return image, target\n\n            # Implementation use 5 iteration to find possible candidate\n            for _ in range(5):\n                # 0.3*0.3 approx. 0.1\n                w = random.uniform(0.3, 1.0)\n                h = random.uniform(0.3, 1.0)\n                if w/h < 0.5 or w/h > 2:  # 保证宽高比例在0.5-2之间\n                    continue\n\n                # left 0 ~ wtot - w, top 0 ~ htot - h\n                left = random.uniform(0, 1.0 - w)\n                top = random.uniform(0, 1.0 - h)\n\n                right = left + w\n                bottom = top + h\n\n                htot = target[TARGET_FIELD_HEIGHT_WIDTH][0]\n                wtot = target[TARGET_FIELD_HEIGHT_WIDTH][1]\n\n                # 裁剪 patch\n                left_idx = int(left * wtot)\n                top_idx = int(top * htot)\n                right_idx = int(right * wtot)\n                bottom_idx = int(bottom * htot)\n                image = image.crop((left_idx, top_idx, right_idx, bottom_idx))\n                # image.save(f\"./output/crop/crop_image{self.count}.jpg\")\n                # self.count += 1\n                return image, target\n\n\nclass Resize(object):\n    \"\"\"对图像进行resize处理,该方法应放在ToTensor前\"\"\"\n    def __init__(self, size=(256, 256)):\n        self.resize = t.Resize(size)\n        # self.count = 0\n\n    def __call__(self, image, target):\n        image = self.resize(image)\n        # image.save(f\"./output/resize/resize{self.count}.jpg\")\n        # self.count += 1\n        return image, target\n\n\nclass ColorJitter(object):\n    \"\"\"对图像颜色信息进行随机调整,该方法应放在ToTensor前\"\"\"\n    def __init__(self, brightness=0.125, contrast=0.5, saturation=0.5, hue=0.05):\n        self.trans = t.ColorJitter(brightness, contrast, saturation, hue)\n        self.count = 0\n\n    def __call__(self, image, target):\n        image = self.trans(image)\n        # image.save(f\"./output/color_jitter/color_jitter{self.count}.jpg\")\n        # self.count += 1\n        return image, target\n\n# normMean = [0.48698336]\n# normStd = [0.23673548]\nclass Normalization(object):\n    \"\"\"对图像标准化处理,该方法应放在ToTensor后\"\"\"\n    def __init__(self, mean=None, std=None):\n        # TODO: 计算训练集的mean和std\n        if mean is None:\n            mean = [0.48698336]\n        if std is None:\n            std = [0.23673548]\n        self.normalize = t.Normalize(mean=mean, std=std)\n\n    def __call__(self, image, target):\n        image = self.normalize(image)\n        return image, target","metadata":{"execution":{"iopub.status.busy":"2023-03-19T06:15:58.683254Z","iopub.execute_input":"2023-03-19T06:15:58.684133Z","iopub.status.idle":"2023-03-19T06:15:58.710122Z","shell.execute_reply.started":"2023-03-19T06:15:58.684074Z","shell.execute_reply":"2023-03-19T06:15:58.708872Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# model.py\n\nimport torch\nfrom torch import nn\n\n# 编码器层\nclass EncoderLayer(nn.Module):\n    def __init__(self, max_len, emb_size):\n        super().__init__()\n        # 多头注意力\n        self.mh = MultiHead(max_len, emb_size)\n        # 全连接输出\n        self.fc = FullyConnectedOutput()\n\n    def forward(self, x):\n        # 计算自注意力,维度不变\n        # [b, 256 + 1, 768] -> [b, 256 + 1, 768]\n        score = self.mh(x, x, x, mask=None)\n\n        # 全连接输出,维度不变\n        # [b, 256 + 1, 768] -> [b, 256 + 1, 768]\n        out = self.fc(score)\n\n        return out\n\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, max_len, emb_size):\n        super().__init__()\n        self.layer_1 = EncoderLayer(max_len, emb_size)\n        self.layer_2 = EncoderLayer(max_len, emb_size)\n        self.layer_3 = EncoderLayer(max_len, emb_size)\n\n    def forward(self, x):\n        x = self.layer_1(x)\n        x = self.layer_2(x)\n        x = self.layer_3(x)\n        return x\n\n# 主模型\nclass TransformerEncoder(torch.nn.Module):\n    def __init__(self, max_len, emb_size):\n        super().__init__()\n        # 位置编码和词嵌入层\n        self.encoder = Encoder(max_len, emb_size)\n\n    def forward(self, x):\n        # 编码层计算\n        # [b, 256 + 1, 768] -> [b, 256 + 1, 768]\n        return self.encoder(x)\n\n\nclass Vit(nn.Sequential):\n    def __init__(self):\n        super(Vit, self).__init__(\n            PatchEmbedding(in_channels=1, patch_size=16, emb_size=768, img_size=256),\n            TransformerEncoder(max_len=(256 // 16) ** 2 + 1, emb_size=768),\n            ClassificationHead(emb_size=768, n_classes=4)\n        )","metadata":{"execution":{"iopub.status.busy":"2023-03-19T06:15:58.712070Z","iopub.execute_input":"2023-03-19T06:15:58.712815Z","iopub.status.idle":"2023-03-19T06:15:58.727349Z","shell.execute_reply.started":"2023-03-19T06:15:58.712778Z","shell.execute_reply":"2023-03-19T06:15:58.726366Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# main.py\n\nimport torch\nfrom PIL import Image\nimport os\nimport json\nimport datetime\n\n# 预测函数\ndef predict(model, img_path):\n    if len(img_path):\n        print(\"img_path is null or empty\")\n        return\n\n    original_img = Image.open(img_path)\n    data_transform = Compose([Resize(),\n                              ToTensor(),\n                              Normalization()])\n    img, _ = data_transform(original_img)\n    # 改为批量预测\n    x = torch.unsqueeze(img, dim=0)\n\n    # x = [1, ]\n    model.eval()\n    pred = model(x)\n\n    # read class_indict\n    json_file = \"./chest_x_ray_classes.json\"\n    assert os.path.exists(json_file), \"{} file not exist.\".format(json_file)\n\n    with open(json_file, 'r') as f:\n        class_dict = json.load(f)\n\n    print(\"Predict result: \" + class_dict[pred])\n\ndef main(args):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    # 1，数据读取后的处理工作\n    #     - 类型转换\n    #     - 数据增强\n    data_transform = {\n        TRAIN_DATA_PATH: Compose([SSDCropping(),  # 图像切割\n                                  Resize(),  # 统一大小\n                                  ColorJitter(),  # 颜色抖动\n                                  ToTensor(),  # 转张量\n                                  RandomHorizontalFlip(),  # 水平翻转\n                                  Normalization()]),  # 标准化\n\n        VAL_DATA_PATH: Compose([Resize(),\n                                ToTensor(),\n                                Normalization()]),\n\n        TEST_DATA_PATH: Compose([Resize(),\n                                 ToTensor(),\n                                 Normalization()])\n    }\n\n    # 构建训练数据集\n    train_loader = get_chest_xray_dataloader(data_root_path=args.data_path,\n                                             data_use_type=TRAIN_DATA_PATH,\n                                             transforms=data_transform[TRAIN_DATA_PATH],\n                                             batch_size=32,\n                                             drop_last=False,\n                                             shuffle=True,\n                                             collate_fn=None)\n\n    # 构建验证数据集\n    val_loader = get_chest_xray_dataloader(data_root_path=args.data_path,\n                                           data_use_type=VAL_DATA_PATH,\n                                           transforms=data_transform[VAL_DATA_PATH],\n                                           batch_size=5,\n                                           drop_last=False,\n                                           shuffle=True,\n                                           collate_fn=None)\n\n    # 构建测试数据集\n    test_loader = get_chest_xray_dataloader(data_root_path=args.data_path,\n                                            data_use_type=TEST_DATA_PATH,\n                                            transforms=data_transform[TEST_DATA_PATH],\n                                            batch_size=32,\n                                            drop_last=False,\n                                            shuffle=True,\n                                            collate_fn=None)\n\n    # 构建模型\n    model = Vit()\n    model.to(device=device)\n    model.train()\n    # 定义损失函数\n    loss_func = torch.nn.CrossEntropyLoss()\n    # 定义优化器\n    optim = torch.optim.Adam(params=model.parameters(), lr=0.00001)\n\n    if len(args.resume) > 0:\n        checkpoint = torch.load(f=args.resume, map_location=\"cpu\")\n        model.load_state_dict(state_dict=checkpoint[\"model\"])\n        optim.load_state_dict(state_dict=checkpoint[\"optimizer\"])\n        args.start_epoch = checkpoint[\"last_epoch\"] + 1\n        print(\"the training process from epoch{}...\".format(args.start_epoch))\n\n    weighted_losses = torch.zeros(1).to(device=device)\n    for epoch in range(args.start_epoch, args.start_epoch + args.epochs):\n        train_acc_list = []\n        val_acc_list = []\n        test_acc_list = []\n        for i, (x, y) in enumerate(train_loader):\n            if len(x) <= 0:\n                continue\n            x = torch.stack(x, dim=0)\n            y = torch.stack(y, dim=0)\n            # x = [B, 256 + 1, 768]\n            x = x.to(device=device)\n            # y = [B, 1]\n            y = y.to(device=device)\n            # pred = [B, 1]\n            pred = model(x)\n\n            loss = loss_func(pred, y.reshape(-1))\n            weighted_losses = (i * weighted_losses + loss ) / (i + 1)\n\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n\n            if i % 20 == 0:\n                lr = optim.param_groups[0]['lr']\n                print(\"{} Epoch{}/{}: lr={}, cur_loss={:.4}, weighted_loss={:.4}\".format(\n                      datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n                      epoch,\n                      i,\n                      lr,\n                      loss.item(),\n                      weighted_losses.item()))\n\n            # [B, 1] -> [B]\n            pred = pred.argmax(dim=-1).reshape(-1)\n            # [B, 1] -> [B]\n            y = y.reshape(-1)\n\n            train_acc_list.extend((pred == y).to(dtype=torch.float32).tolist())\n\n        # save weights\n        save_files = {\n            'model': model.state_dict(),\n            'optimizer': optim.state_dict(),\n            'last_epoch': epoch}\n        torch.save(save_files, \"./save_weights/ChestXRay-{}.pth\".format(epoch))\n\n        # 验证\n        for i, (x, y) in enumerate(val_loader):\n            if len(x) <= 0:\n                continue\n            \n            x = torch.stack(x, dim=0)\n            y = torch.stack(y, dim=0)\n            \n            x = x.to(device=device)\n            y = y.to(device=device)\n            \n            pred_val = model(x).argmax(dim=-1).reshape(-1)\n            y = y.reshape(-1)\n            val_acc_list.extend((pred_val == y).to(dtype=torch.float32).tolist())\n\n        print(\"Epoch {}: train_acc={:4}/{}, val_acc={:4}/{}, lr={:6}\".format(\n            epoch,\n            sum(train_acc_list) / len(train_acc_list),\n            len(train_acc_list),\n            sum(val_acc_list) / len(val_acc_list),\n            len(val_acc_list),\n            optim.param_groups[0][\"lr\"]))\n\n    # 测试\n    for i, (x, y) in enumerate(test_loader):\n        if len(x) <= 0:\n            continue\n            \n        x = torch.stack(x, dim=0)\n        y = torch.stack(y, dim=0)\n            \n        x = x.to(device=device)\n        y = y.to(device=device)\n        \n        pred_test = model(x).argmax(dim=-1).reshape(-1)\n        y = y.reshape(-1)\n        test_acc_list.extend((pred_test == y).to(dtype=torch.float32).tolist())\n\n    print(\"Epoch {}-{}: test_acc={:4}/{}\".format(\n          args.start_epoch,\n          args.start_epoch + args.epochs,\n          sum(test_acc_list) / len(test_acc_list),\n          len(test_acc_list)))\n\nif __name__ == '__main__':\n\n    import argparse\n\n    parser = argparse.ArgumentParser(description=__doc__)\n\n    # 检测的目标类别个数，不包括背景(替换：自己的检测类别)\n    parser.add_argument('--num_classes', default=4, type=int, help='num_classes')\n    # 训练数据集的根目录\n    parser.add_argument('--data_path', default='/kaggle/input/chest-xray-pneumoniacovid19tuberculosis', help='dataset')\n    # 文件保存地址\n    parser.add_argument('--output-dir', default='./save_weights', help='path where to save')\n    # 若需要接着上次训练，则指定上次训练保存权重文件地址 /kaggle/working/save_weights/ChestXRay-5.pth\n    parser.add_argument('--resume', default='/kaggle/working/save_weights/ChestXRay-24.pth', type=str, help='resume from checkpoint')\n    # 指定接着从哪个epoch数开始训练\n    parser.add_argument('--start_epoch', default=25, type=int, help='start epoch')\n    # 训练的总epoch数\n    parser.add_argument('--epochs', default=15, type=int, metavar='N',\n                        help='number of total epochs to run')\n    # 训练的batch size\n    parser.add_argument('--batch_size', default=32, type=int, metavar='N',\n                        help='batch size when training.')\n\n    args = parser.parse_args(args=[])\n\n    # 检查保存权重文件夹是否存在，不存在则创建\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n        \n    main(args)","metadata":{"execution":{"iopub.status.busy":"2023-03-19T06:47:12.314914Z","iopub.execute_input":"2023-03-19T06:47:12.315967Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"the training process from epoch25...\n2023-03-19 06:47:13.668940 Epoch25/0: lr=1e-05, cur_loss=0.5735, weighted_loss=0.5735\n2023-03-19 06:47:27.861850 Epoch25/20: lr=1e-05, cur_loss=0.3735, weighted_loss=0.4313\n2023-03-19 06:47:41.683630 Epoch25/40: lr=1e-05, cur_loss=0.7257, weighted_loss=0.4344\n2023-03-19 06:47:54.066411 Epoch25/60: lr=1e-05, cur_loss=0.2973, weighted_loss=0.4217\n2023-03-19 06:48:07.384472 Epoch25/80: lr=1e-05, cur_loss=0.274, weighted_loss=0.4079\n2023-03-19 06:48:20.670090 Epoch25/100: lr=1e-05, cur_loss=0.2656, weighted_loss=0.4011\n2023-03-19 06:48:33.059486 Epoch25/120: lr=1e-05, cur_loss=0.3566, weighted_loss=0.3979\n2023-03-19 06:48:47.376959 Epoch25/140: lr=1e-05, cur_loss=0.3241, weighted_loss=0.3909\n2023-03-19 06:49:00.459987 Epoch25/160: lr=1e-05, cur_loss=0.3136, weighted_loss=0.3947\n2023-03-19 06:49:14.161537 Epoch25/180: lr=1e-05, cur_loss=0.5902, weighted_loss=0.3958\nEpoch 25: train_acc=0.8515649699652229/6326, val_acc=0.6578947368421053/38, lr= 1e-05\n2023-03-19 06:49:26.800559 Epoch26/0: lr=1e-05, cur_loss=0.5495, weighted_loss=0.5495\n2023-03-19 06:49:40.343307 Epoch26/20: lr=1e-05, cur_loss=0.3405, weighted_loss=0.3829\n2023-03-19 06:49:53.790340 Epoch26/40: lr=1e-05, cur_loss=0.6991, weighted_loss=0.399\n2023-03-19 06:50:07.040559 Epoch26/60: lr=1e-05, cur_loss=0.5067, weighted_loss=0.3911\n2023-03-19 06:50:20.282159 Epoch26/80: lr=1e-05, cur_loss=0.3144, weighted_loss=0.379\n2023-03-19 06:50:33.088781 Epoch26/100: lr=1e-05, cur_loss=0.5358, weighted_loss=0.3831\n2023-03-19 06:50:46.015213 Epoch26/120: lr=1e-05, cur_loss=0.6042, weighted_loss=0.392\n2023-03-19 06:50:59.912599 Epoch26/140: lr=1e-05, cur_loss=0.2998, weighted_loss=0.3962\n2023-03-19 06:51:13.044010 Epoch26/160: lr=1e-05, cur_loss=0.2075, weighted_loss=0.3938\n2023-03-19 06:51:26.251481 Epoch26/180: lr=1e-05, cur_loss=0.4622, weighted_loss=0.4007\nEpoch 26: train_acc=0.848719570028454/6326, val_acc=0.8157894736842105/38, lr= 1e-05\n2023-03-19 06:51:39.089992 Epoch27/0: lr=1e-05, cur_loss=0.5307, weighted_loss=0.5307\n2023-03-19 06:51:52.045280 Epoch27/20: lr=1e-05, cur_loss=0.2413, weighted_loss=0.3829\n2023-03-19 06:52:04.717428 Epoch27/40: lr=1e-05, cur_loss=0.4301, weighted_loss=0.3902\n2023-03-19 06:52:17.882183 Epoch27/60: lr=1e-05, cur_loss=0.3346, weighted_loss=0.3795\n2023-03-19 06:52:30.559235 Epoch27/80: lr=1e-05, cur_loss=0.2517, weighted_loss=0.3753\n2023-03-19 06:52:42.557482 Epoch27/100: lr=1e-05, cur_loss=0.2382, weighted_loss=0.3652\n2023-03-19 06:52:56.283160 Epoch27/120: lr=1e-05, cur_loss=0.4161, weighted_loss=0.3692\n2023-03-19 06:53:10.765138 Epoch27/140: lr=1e-05, cur_loss=0.2924, weighted_loss=0.3761\n2023-03-19 06:53:23.710814 Epoch27/160: lr=1e-05, cur_loss=0.2462, weighted_loss=0.3844\n2023-03-19 06:53:37.995885 Epoch27/180: lr=1e-05, cur_loss=0.3267, weighted_loss=0.3842\nEpoch 27: train_acc=0.8585203920328802/6326, val_acc=0.8157894736842105/38, lr= 1e-05\n2023-03-19 06:53:50.628795 Epoch28/0: lr=1e-05, cur_loss=0.3321, weighted_loss=0.3321\n2023-03-19 06:54:04.052538 Epoch28/20: lr=1e-05, cur_loss=0.396, weighted_loss=0.3735\n2023-03-19 06:54:16.369569 Epoch28/40: lr=1e-05, cur_loss=0.3239, weighted_loss=0.375\n2023-03-19 06:54:29.701368 Epoch28/60: lr=1e-05, cur_loss=0.3624, weighted_loss=0.3668\n2023-03-19 06:54:42.863109 Epoch28/80: lr=1e-05, cur_loss=0.1305, weighted_loss=0.3637\n2023-03-19 06:54:55.828894 Epoch28/100: lr=1e-05, cur_loss=0.3122, weighted_loss=0.3627\n2023-03-19 06:55:09.343854 Epoch28/120: lr=1e-05, cur_loss=0.5946, weighted_loss=0.369\n2023-03-19 06:55:22.487787 Epoch28/140: lr=1e-05, cur_loss=0.3776, weighted_loss=0.3695\n2023-03-19 06:55:36.055899 Epoch28/160: lr=1e-05, cur_loss=0.3437, weighted_loss=0.3714\n2023-03-19 06:55:49.920651 Epoch28/180: lr=1e-05, cur_loss=0.3349, weighted_loss=0.3713\nEpoch 28: train_acc=0.8583623142586152/6326, val_acc=0.7894736842105263/38, lr= 1e-05\n2023-03-19 06:56:02.694450 Epoch29/0: lr=1e-05, cur_loss=0.5262, weighted_loss=0.5262\n2023-03-19 06:56:16.389127 Epoch29/20: lr=1e-05, cur_loss=0.3734, weighted_loss=0.4415\n2023-03-19 06:56:29.141036 Epoch29/40: lr=1e-05, cur_loss=0.4203, weighted_loss=0.4132\n2023-03-19 06:56:42.076910 Epoch29/60: lr=1e-05, cur_loss=0.3406, weighted_loss=0.4102\n2023-03-19 06:56:54.855976 Epoch29/80: lr=1e-05, cur_loss=0.5177, weighted_loss=0.4037\n2023-03-19 06:57:08.803079 Epoch29/100: lr=1e-05, cur_loss=0.4202, weighted_loss=0.3988\n2023-03-19 06:57:21.694014 Epoch29/120: lr=1e-05, cur_loss=0.2847, weighted_loss=0.3957\n2023-03-19 06:57:33.990390 Epoch29/140: lr=1e-05, cur_loss=0.4579, weighted_loss=0.3853\n2023-03-19 06:57:47.900875 Epoch29/160: lr=1e-05, cur_loss=0.2905, weighted_loss=0.3842\n2023-03-19 06:58:00.452400 Epoch29/180: lr=1e-05, cur_loss=0.345, weighted_loss=0.3844\nEpoch 29: train_acc=0.856465380967436/6326, val_acc=0.7631578947368421/38, lr= 1e-05\n2023-03-19 06:58:14.036203 Epoch30/0: lr=1e-05, cur_loss=0.1906, weighted_loss=0.1906\n2023-03-19 06:58:27.085614 Epoch30/20: lr=1e-05, cur_loss=0.4523, weighted_loss=0.349\n2023-03-19 06:58:40.181985 Epoch30/40: lr=1e-05, cur_loss=0.4368, weighted_loss=0.3727\n2023-03-19 06:58:53.272745 Epoch30/60: lr=1e-05, cur_loss=0.205, weighted_loss=0.3692\n2023-03-19 06:59:06.775279 Epoch30/80: lr=1e-05, cur_loss=0.3273, weighted_loss=0.3704\n2023-03-19 06:59:20.829229 Epoch30/100: lr=1e-05, cur_loss=0.3949, weighted_loss=0.3821\n2023-03-19 06:59:33.183301 Epoch30/120: lr=1e-05, cur_loss=0.2289, weighted_loss=0.3833\n2023-03-19 06:59:47.368561 Epoch30/140: lr=1e-05, cur_loss=0.2748, weighted_loss=0.3779\n2023-03-19 06:59:59.800035 Epoch30/160: lr=1e-05, cur_loss=0.6123, weighted_loss=0.3766\n2023-03-19 07:00:12.304504 Epoch30/180: lr=1e-05, cur_loss=0.2212, weighted_loss=0.3739\nEpoch 30: train_acc=0.8624723363895036/6326, val_acc=0.8157894736842105/38, lr= 1e-05\n2023-03-19 07:00:25.371288 Epoch31/0: lr=1e-05, cur_loss=0.3237, weighted_loss=0.3237\n2023-03-19 07:00:37.983740 Epoch31/20: lr=1e-05, cur_loss=0.3634, weighted_loss=0.3398\n2023-03-19 07:00:51.951417 Epoch31/40: lr=1e-05, cur_loss=0.2101, weighted_loss=0.3525\n2023-03-19 07:01:04.298965 Epoch31/60: lr=1e-05, cur_loss=0.3568, weighted_loss=0.37\n2023-03-19 07:01:17.686673 Epoch31/80: lr=1e-05, cur_loss=0.3766, weighted_loss=0.3767\n2023-03-19 07:01:31.205865 Epoch31/100: lr=1e-05, cur_loss=0.3957, weighted_loss=0.3765\n2023-03-19 07:01:44.021339 Epoch31/120: lr=1e-05, cur_loss=0.342, weighted_loss=0.3674\n2023-03-19 07:01:56.735120 Epoch31/140: lr=1e-05, cur_loss=0.1722, weighted_loss=0.3698\n2023-03-19 07:02:09.903278 Epoch31/160: lr=1e-05, cur_loss=0.4197, weighted_loss=0.3761\n2023-03-19 07:02:24.223774 Epoch31/180: lr=1e-05, cur_loss=0.4046, weighted_loss=0.3771\nEpoch 31: train_acc=0.8604173253240595/6326, val_acc=0.7631578947368421/38, lr= 1e-05\n2023-03-19 07:02:36.690201 Epoch32/0: lr=1e-05, cur_loss=0.4595, weighted_loss=0.4595\n2023-03-19 07:02:49.328886 Epoch32/20: lr=1e-05, cur_loss=0.4143, weighted_loss=0.3931\n2023-03-19 07:03:02.730467 Epoch32/40: lr=1e-05, cur_loss=0.2305, weighted_loss=0.3732\n2023-03-19 07:03:16.320502 Epoch32/60: lr=1e-05, cur_loss=0.3534, weighted_loss=0.3781\n2023-03-19 07:03:30.283677 Epoch32/80: lr=1e-05, cur_loss=0.1343, weighted_loss=0.3823\n2023-03-19 07:03:42.720871 Epoch32/100: lr=1e-05, cur_loss=0.4865, weighted_loss=0.38\n2023-03-19 07:03:55.903822 Epoch32/120: lr=1e-05, cur_loss=0.2551, weighted_loss=0.3846\n2023-03-19 07:04:10.065547 Epoch32/140: lr=1e-05, cur_loss=0.129, weighted_loss=0.3912\n2023-03-19 07:04:23.461264 Epoch32/160: lr=1e-05, cur_loss=0.3756, weighted_loss=0.39\n2023-03-19 07:04:35.918193 Epoch32/180: lr=1e-05, cur_loss=0.2206, weighted_loss=0.392\nEpoch 32: train_acc=0.8580461587100854/6326, val_acc=0.7631578947368421/38, lr= 1e-05\n2023-03-19 07:04:48.513366 Epoch33/0: lr=1e-05, cur_loss=0.4931, weighted_loss=0.4931\n2023-03-19 07:05:01.800147 Epoch33/20: lr=1e-05, cur_loss=0.3857, weighted_loss=0.3377\n2023-03-19 07:05:14.823352 Epoch33/40: lr=1e-05, cur_loss=0.5867, weighted_loss=0.363\n2023-03-19 07:05:27.145077 Epoch33/60: lr=1e-05, cur_loss=0.6224, weighted_loss=0.3828\n2023-03-19 07:05:40.465882 Epoch33/80: lr=1e-05, cur_loss=0.5292, weighted_loss=0.3889\n2023-03-19 07:05:53.458594 Epoch33/100: lr=1e-05, cur_loss=0.2576, weighted_loss=0.3776\n2023-03-19 07:06:06.602230 Epoch33/120: lr=1e-05, cur_loss=0.3109, weighted_loss=0.3714\n2023-03-19 07:06:19.827778 Epoch33/140: lr=1e-05, cur_loss=0.218, weighted_loss=0.3802\n2023-03-19 07:06:32.650426 Epoch33/160: lr=1e-05, cur_loss=0.3907, weighted_loss=0.3762\n2023-03-19 07:06:48.149820 Epoch33/180: lr=1e-05, cur_loss=0.3953, weighted_loss=0.3768\nEpoch 33: train_acc=0.8577300031615555/6326, val_acc=0.7894736842105263/38, lr= 1e-05\n2023-03-19 07:07:00.823273 Epoch34/0: lr=1e-05, cur_loss=0.2508, weighted_loss=0.2508\n2023-03-19 07:07:13.736207 Epoch34/20: lr=1e-05, cur_loss=0.6177, weighted_loss=0.3735\n2023-03-19 07:07:26.771530 Epoch34/40: lr=1e-05, cur_loss=0.5705, weighted_loss=0.3864\n2023-03-19 07:07:40.848109 Epoch34/60: lr=1e-05, cur_loss=0.2037, weighted_loss=0.3865\n2023-03-19 07:07:53.978083 Epoch34/80: lr=1e-05, cur_loss=0.3894, weighted_loss=0.383\n2023-03-19 07:08:06.871321 Epoch34/100: lr=1e-05, cur_loss=0.4037, weighted_loss=0.3863\n2023-03-19 07:08:21.001558 Epoch34/120: lr=1e-05, cur_loss=0.3299, weighted_loss=0.3838\n2023-03-19 07:08:33.836830 Epoch34/140: lr=1e-05, cur_loss=0.4309, weighted_loss=0.378\n2023-03-19 07:08:45.947031 Epoch34/160: lr=1e-05, cur_loss=0.4782, weighted_loss=0.3764\n2023-03-19 07:08:58.104439 Epoch34/180: lr=1e-05, cur_loss=0.42, weighted_loss=0.3802\nEpoch 34: train_acc=0.8601011697755295/6326, val_acc=0.7894736842105263/38, lr= 1e-05\n2023-03-19 07:09:11.668034 Epoch35/0: lr=1e-05, cur_loss=0.2182, weighted_loss=0.2182\n2023-03-19 07:09:24.532140 Epoch35/20: lr=1e-05, cur_loss=0.2774, weighted_loss=0.3546\n2023-03-19 07:09:38.012135 Epoch35/40: lr=1e-05, cur_loss=0.4257, weighted_loss=0.3646\n2023-03-19 07:09:51.701848 Epoch35/60: lr=1e-05, cur_loss=0.4559, weighted_loss=0.3813\n2023-03-19 07:10:05.752300 Epoch35/80: lr=1e-05, cur_loss=0.3764, weighted_loss=0.3711\n2023-03-19 07:10:18.623507 Epoch35/100: lr=1e-05, cur_loss=0.5092, weighted_loss=0.3764\n2023-03-19 07:10:31.655746 Epoch35/120: lr=1e-05, cur_loss=0.3932, weighted_loss=0.3717\n2023-03-19 07:10:43.981511 Epoch35/140: lr=1e-05, cur_loss=0.2517, weighted_loss=0.3656\n2023-03-19 07:10:57.801977 Epoch35/160: lr=1e-05, cur_loss=0.4385, weighted_loss=0.3708\n2023-03-19 07:11:10.782903 Epoch35/180: lr=1e-05, cur_loss=0.4719, weighted_loss=0.3696\nEpoch 35: train_acc=0.8605754030983244/6326, val_acc=0.8421052631578947/38, lr= 1e-05\n2023-03-19 07:11:23.382200 Epoch36/0: lr=1e-05, cur_loss=0.3987, weighted_loss=0.3987\n2023-03-19 07:11:35.633865 Epoch36/20: lr=1e-05, cur_loss=0.2412, weighted_loss=0.3902\n2023-03-19 07:11:48.923802 Epoch36/40: lr=1e-05, cur_loss=0.487, weighted_loss=0.3678\n2023-03-19 07:12:03.370033 Epoch36/60: lr=1e-05, cur_loss=0.2516, weighted_loss=0.3624\n2023-03-19 07:12:16.823472 Epoch36/80: lr=1e-05, cur_loss=0.6257, weighted_loss=0.3655\n2023-03-19 07:12:29.959302 Epoch36/100: lr=1e-05, cur_loss=0.4519, weighted_loss=0.3645\n2023-03-19 07:12:43.821533 Epoch36/120: lr=1e-05, cur_loss=0.2825, weighted_loss=0.3669\n","output_type":"stream"}]}]}